<!-- Copyright 2022

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->

<p align="justify">
    Welcome to the comparison page. Here you are able to compare two tasks from the same survey, using one as the ground truth. The comparison is divided into three 
    sections: summary statistics, species statistics, and a comparison matrix.
</p>

<p align="justify">
    The summary statistics give you the overall picture including the number of matched and unmatched sightings between the surveys, and final recall and precision rates
    for your non-ground-truth task. You are also shown the number of sightings marked as unknown in this task, affecting the accuracy of the comparison in the case that 
    a different person was used to tag your two tasks. The errors are then broken down into two categories: sightings missed by the AI animal detector (applicable when 
    you have imported your results from an external source), and empty images 
    incorrectly clustered into clusters containing animals. These are the two primary sources of error in the approach used by TrapTagger, meaning that 
    the remaining errors are most likely due to human error.
</p>

<p align="justify">
    The species-statistics section then gives you species-level recall and precision rates. This allows you to see the performance of the system for your species of interest, 
    and whether the system is under-performing on any particular species.
</p>

<p align="justify">
    Lastly, the comparison matrix allows you to see exactly where the errors lie by showing you the number of sightings with each possible combination of labels from the two 
    tasks. The highlighted diagonal shows the number of sightings where there is agreement between the two tasks, whilst the other cells show the unmatched sightings. Further, there 
    is a separate cell below the table containing the multiple mismatches, where there is more than one disagreement in an image, and its not possible to categorise
    the errors into a single cell in the main table. Overall, this display allows you to do a more in-depth comparison of the two tasks, in turn allowing you, for example,  
    to determine if there has been a confusion between two species by one of the taggers. Moreover, you are able to click on the cells to view the associated images and their 
    labels. This allows you to see where the two tasks differ, and more importantly, allows you to determine which task is incorrect in each case. It is this final step that 
    is a vital part of the comparison process, because inevitably your ground-truth task will contain human errors as well. These errors essentially cause a double effect as they 
    reduce the performance statistics of your TrapTagger task, and mean that you are not comparing these statistics to 100% recall and precision rates, affecting the perceived performance.
</p>